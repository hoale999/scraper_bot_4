import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
import json
import re
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_
from selenium.webdriver.common.by import By
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support import expected_conditions as EC
from urllib3.util.retry import Retry

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX (Gi·ªØ nguy√™n t·ª´ c√°c bot tr∆∞·ªõc) ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

current_year = datetime.now().year

def fetch_sbt_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SBT (NƒÉm {current_year}) ---")
    
    base_url = "https://ttcagris.com.vn"
    # Danh s√°ch URL theo y√™u c·∫ßu c·ªßa b·∫°n
    targets = [
        ("SBT - ƒêHƒêCƒê Th∆∞·ªùng ni√™n", f"{base_url}/quan-he-nha-dau-tu/dai-hoi-dong-co-dong?year={current_year}&cate=1"),
        ("SBT - ƒêHƒêCƒê B·∫•t th∆∞·ªùng",  f"{base_url}/quan-he-nha-dau-tu/dai-hoi-dong-co-dong?year={current_year}&cate=2"),
        ("SBT - ƒêHƒêCƒê L·∫•y √Ω ki·∫øn",  f"{base_url}/quan-he-nha-dau-tu/dai-hoi-dong-co-dong?year={current_year}&cate=3"),
        ("SBT - BCTC Ki·ªÉm to√°n",    f"{base_url}/quan-he-nha-dau-tu/bao-cao-tai-chinh?year={current_year}&cate=1"),
        ("SBT - BCTC Qu√Ω",          f"{base_url}/quan-he-nha-dau-tu/bao-cao-tai-chinh?year={current_year}&cate=3"),
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=15, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T V2: FIND BY DATE ---
            # 1. T√¨m t·∫•t c·∫£ c√°c text node c√≥ ƒë·ªãnh d·∫°ng ng√†y dd/mm/yyyy
            # Regex: t√¨m chu·ªói c√≥ 2 s·ªë / 2 s·ªë / 4 s·ªë
            date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
            
            # N·∫øu kh√¥ng t√¨m th·∫•y b·∫±ng ng√†y, th·ª≠ t√¨m c√°c th·∫ª div c√≥ class ch·ª©a 'row' ho·∫∑c 'item' (Backup)
            if not date_nodes:
                # print(f"   ‚ö†Ô∏è Kh√¥ng th·∫•y ng√†y th√°ng t·∫°i {source_label}, th·ª≠ backup...")
                pass

            for node in date_nodes:
                try:
                    date_str = node.strip()
                    # L·ªçc ch√≠nh x√°c chu·ªói ng√†y (ƒë√¥i khi n√≥ n·∫±m l·∫´n trong text d√†i)
                    match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                    if not match: continue
                    clean_date = match.group(1)

                    # 2. T·ª´ node ng√†y, t√¨m parent l√† d√≤ng ch·ª©a tin (th∆∞·ªùng l√† tr, li, ho·∫∑c div)
                    # Ta t√¨m th·∫ª cha g·∫ßn nh·∫•t c√≥ ch·ª©a th·∫ª <a>
                    container = node.find_parent(['tr', 'div', 'li', 'p'])
                    
                    if not container: continue

                    # 3. T√¨m link & Title trong container ƒë√≥
                    link_tag = container.find('a')
                    
                    # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: ƒê√¥i khi date n·∫±m TRONG th·∫ª a, ho·∫∑c th·∫ª a n·∫±m b√™n c·∫°nh
                    if not link_tag:
                        # Th·ª≠ t√¨m th·∫ª a ·ªü c·∫•p cao h∆°n m·ªôt ch√∫t (√¥ng n·ªôi)
                        container = container.parent
                        if container:
                            link_tag = container.find('a')

                    if not link_tag: continue

                    link = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)

                    # L√†m s·∫°ch d·ªØ li·ªáu
                    if not link: continue
                    if link.startswith('/'): link = base_url + link
                    
                    # Lo·∫°i b·ªè c√°c link r√°c n·∫øu v·∫´n l·ªçt l∆∞·ªõi (Check ƒë·ªô d√†i title)
                    if len(title) < 5: continue 
                    if "facebook" in link.lower() or "youtube" in link.lower(): continue

                    # 4. L∆∞u k·∫øt qu·∫£
                    item_id = link
                    if item_id in seen_ids:
                        continue
                    
                    # N·∫øu ƒëang filter nƒÉm 2025 m√† web tr·∫£ v·ªÅ tin c≈© th√¨ b·ªè qua (tu·ª≥ ch·ªçn)
                    # if str(current_year) not in clean_date: continue 

                    new_items.append({
                        "source": source_label,
                        "id": item_id,
                        "title": title,
                        "date": clean_date,
                        "link": link
                    })
                    seen_ids.add(item_id)

                except Exception as e:
                    continue

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_vgc_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VGC (NƒÉm {current_year}) ---")
    
    base_url = "https://www.viglacera.com.vn"
    targets = [
        ("VGC - B√°o c√°o t√†i ch√≠nh", f"{base_url}/document-category/bao-cao-tai-chinh"),
        ("VGC - B√°o c√°o th∆∞·ªùng ni√™n", f"{base_url}/document-category/bao-cao-thuong-nien"),
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=15, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T 1: T√åM THEO NG√ÄY (∆Øu ti√™n) ---
            # T√¨m text d·∫°ng dd/mm/yyyy
            date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
            
            # Danh s√°ch t·∫°m ƒë·ªÉ check tr√πng trong loop n√†y
            found_in_pass_1 = False

            if date_nodes:
                for node in date_nodes:
                    try:
                        date_str = node.strip()
                        match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                        if not match: continue
                        clean_date = match.group(1)

                        # T·ª´ ng√†y -> t√¨m ng∆∞·ª£c ra th·∫ª cha ch·ª©a Link
                        # Th·ª≠ c√°c th·∫ª bao ph·ªï bi·∫øn: div, li, tr, article
                        container = node.find_parent(['div', 'li', 'tr', 'article'])
                        if not container: continue

                        link_tag = container.find('a')
                        if not link_tag: continue

                        link = link_tag.get('href', '')
                        title = link_tag.get_text(strip=True)

                        if not link: continue
                        if link.startswith('/'): link = base_url + link

                        # Validate
                        if len(title) < 5: continue
                        
                        item_id = link
                        if item_id in seen_ids: continue

                        # Ch·ªâ l·∫•y nƒÉm hi·ªán t·∫°i (n·∫øu c·∫ßn thi·∫øt)
                        # if str(current_year) not in clean_date and str(current_year) not in title: continue

                        new_items.append({
                            "source": source_label,
                            "id": item_id,
                            "title": title,
                            "date": clean_date,
                            "link": link
                        })
                        seen_ids.add(item_id)
                        found_in_pass_1 = True

                    except Exception:
                        continue
            
            # --- CHI·∫æN THU·∫¨T 2: QU√âT LINK CH·ª®A NƒÇM (D·ª± ph√≤ng) ---
            # N·∫øu chi·∫øn thu·∫≠t 1 kh√¥ng ra k·∫øt qu·∫£ n√†o (do web ·∫©n ng√†y ho·∫∑c format l·∫°),
            # ta t√¨m c√°c link c√≥ title ch·ª©a "2025" (current_year)
            if not found_in_pass_1:
                # print(f"   ‚ö†Ô∏è VGC: Kh√¥ng th·∫•y ng√†y t·∫°i {source_label}, chuy·ªÉn sang qu√©t Title...")
                all_links = soup.find_all('a')
                for a in all_links:
                    title = a.get_text(strip=True)
                    link = a.get('href', '')
                    
                    if not link or len(title) < 10: continue
                    
                    # ƒêi·ªÅu ki·ªán: Title ph·∫£i ch·ª©a NƒÉm hi·ªán t·∫°i
                    if str(current_year) in title:
                        if link.startswith('/'): link = base_url + link
                        
                        item_id = link
                        if item_id in seen_ids: continue
                        
                        # Gi·∫£ l·∫≠p ng√†y v√¨ kh√¥ng l·∫•y ƒë∆∞·ª£c
                        fake_date = f"01/01/{current_year}"

                        new_items.append({
                            "source": source_label,
                            "id": item_id,
                            "title": title,
                            "date": fake_date,
                            "link": link
                        })
                        seen_ids.add(item_id)

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_shs_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SHS (NƒÉm {current_year}) ---")
    
    targets = [
        ("SHS - B√°o c√°o t√†i ch√≠nh", "https://www.shs.com.vn/quan-he-co-dong/bao-cao-dinh-ky/TAICHINH"),
        ("SHS - ƒêHƒêCƒê", "https://dhcd.shs.com.vn/") 
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # Chi·∫øn thu·∫≠t: Qu√©t text Ng√†y th√°ng (dd/mm/yyyy) -> Neo ng∆∞·ª£c ra Link
            date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
            
            for node in date_nodes:
                try:
                    date_str = node.strip()
                    match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                    if not match: continue
                    clean_date = match.group(1)

                    # T√¨m container ch·ª©a link
                    container = node.find_parent(['tr', 'div', 'li', 'article', 'td'])
                    if not container: continue

                    link_tag = container.find('a')
                    
                    # N·∫øu container hi·ªán t·∫°i kh√¥ng c√≥ a, th·ª≠ nh·∫£y l√™n 1 c·∫•p n·ªØa (tr∆∞·ªùng h·ª£p table td)
                    if not link_tag:
                         container = container.parent
                         if container: link_tag = container.find('a')
                    
                    if not link_tag: continue

                    link = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)

                    if not link: continue
                    # X·ª≠ l√Ω link t∆∞∆°ng ƒë·ªëi
                    if link.startswith('/'): 
                        # V·ªõi trang dhcd.shs.com.vn th√¨ base l√† dhcd...
                        if "dhcd.shs" in url:
                            link = "https://dhcd.shs.com.vn" + link
                        else:
                            link = "https://www.shs.com.vn" + link

                    # Validate r√°c
                    if len(title) < 5: continue
                    
                    item_id = link
                    if item_id in seen_ids: continue

                    # L·ªçc nƒÉm (ch·ªâ l·∫•y tin nƒÉm hi·ªán t·∫°i)
                    if str(current_year) not in clean_date: continue

                    new_items.append({
                        "source": source_label,
                        "id": item_id,
                        "title": title,
                        "date": clean_date,
                        "link": link
                    })
                    seen_ids.add(item_id)

                except Exception:
                    continue

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_mbs_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t MBS (Selenium V2 - NƒÉm {current_year}) ---")
    
    # URL c·∫≠p nh·∫≠t theo c·∫•u tr√∫c th·ª±c t·∫ø th∆∞·ªùng g·∫∑p
    targets = [
        ("MBS - Tin c·ªï ƒë√¥ng", "https://www.mbs.com.vn/tin-co-dong/"),
        ("MBS - B√°o c√°o t√†i ch√≠nh", "https://www.mbs.com.vn/bao-cao-tai-chinh/")
    ]

    new_items = []
    
    # --- C·∫§U H√åNH ANT-DETECT ---
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # Quan tr·ªçng: T·∫Øt t√≠nh nƒÉng b√°o hi·ªáu l√† Bot
    chrome_options.add_argument("--disable-blink-features=AutomationControlled") 
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
    except Exception as e:
        print(f"[MBS] L·ªói kh·ªüi t·∫°o Driver: {e}")
        return []

    try:
        for source_label, url in targets:
            try:
                # print(f"   >> ƒêang truy c·∫≠p: {source_label}...")
                driver.get(url)
                
                # Ch·ªù t·ªëi ƒëa 15s ƒë·ªÉ th·∫ª 'body' t·∫£i xong
                WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                
                # DEBUG: In ra title c·ªßa trang ƒë·ªÉ ch·∫Øc ch·∫Øn ƒë√£ v√†o ƒë∆∞·ª£c
                page_title = driver.title
                # print(f"      [Debug] Page Title: {page_title}")

                # Scroll nh·∫π ƒë·ªÉ trigger load (n·∫øu c√≥)
                driver.execute_script("window.scrollTo(0, 1000);")
                time.sleep(3) # Ch·ªù render

                # L·∫•y HTML
                html_source = driver.page_source
                soup = BeautifulSoup(html_source, 'html.parser')

                # --- CHI·∫æN THU·∫¨T QU√âT "V√âT C·∫†N" ---
                # MBS th∆∞·ªùng b·ªçc tin trong c√°c th·∫ª c√≥ class: 'news-item', 'item', 'row', 'doc-item'
                # Ta s·∫Ω t√¨m t·∫•t c·∫£ th·∫ª <a> v√† check ƒëi·ªÅu ki·ªán
                
                all_links = soup.find_all('a')
                # print(f"      [Debug] T√¨m th·∫•y {len(all_links)} th·∫ª <a>")

                count_added = 0
                for link_tag in all_links:
                    link = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)
                    
                    # N·∫øu title tr·ªëng, l·∫•y attribute title
                    if not title: title = link_tag.get('title', '')
                    
                    if not link or len(title) < 10: continue

                    # B·ªè qua link header/footer/menu
                    if "facebook" in link or "youtube" in link or "mailto" in link: continue

                    # --- X·ª≠ l√Ω Ng√†y th√°ng ---
                    date_str = ""
                    
                    # C√°ch 1: T√¨m th·∫ª ng√†y l√† anh em ho·∫∑c con ch√°u c·ªßa th·∫ª link n√†y
                    # (Th∆∞·ªùng g·∫∑p: <div> <a>Title</a> <span class='date'>...</span> </div>)
                    container = link_tag.find_parent(['div', 'li', 'tr', 'article'])
                    
                    if container:
                        container_text = container.get_text(" ", strip=True)
                        match = re.search(r"(\d{2}/\d{2}/\d{4})", container_text)
                        if match:
                            date_str = match.group(1)
                    
                    # C√°ch 2: N·∫øu kh√¥ng th·∫•y ng√†y, check Title c√≥ ch·ª©a nƒÉm hi·ªán t·∫°i kh√¥ng
                    if not date_str:
                        if str(current_year) in title:
                            date_str = f"01/01/{current_year}"

                    if not date_str: continue # Kh√¥ng c√≥ ng√†y -> B·ªè

                    # --- Validate ---
                    if str(current_year) not in date_str and str(current_year) not in title:
                        continue
                    
                    if not link.startswith('http'):
                        link = "https://www.mbs.com.vn" + link

                    if link in seen_ids: continue

                    new_items.append({
                        "source": source_label,
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    seen_ids.add(link)
                    count_added += 1

                # print(f"      => L·∫•y ƒë∆∞·ª£c {count_added} tin.")

            except Exception as e:
                print(f"   ! L·ªói x·ª≠ l√Ω {source_label}: {e}")

    finally:
        driver.quit()

    return new_items

def fetch_dxg_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t DXG (NƒÉm {current_year}) ---")
    
    url = "https://ir.datxanh.vn/cong-bo-thong-tin"
    
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    try:
        resp = session.get(url, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(resp.content, 'html.parser')

        # Chi·∫øn thu·∫≠t: T√¨m Date (dd/mm/yyyy) -> T√¨m d√≤ng ch·ª©a n√≥ -> T√¨m Link
        date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))

        for node in date_nodes:
            try:
                date_str = node.strip()
                match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                if not match: continue
                clean_date = match.group(1)

                # T√¨m th·∫ª bao ngo√†i (Row ho·∫∑c Item)
                container = node.find_parent(['tr', 'div', 'li', 'article'])
                if not container: continue

                # T√¨m Link & Title
                link_tag = container.find('a')
                
                # N·∫øu kh√¥ng th·∫•y th·∫ª a ngay, th·ª≠ t√¨m r·ªông ra 1 c·∫•p
                if not link_tag:
                    container = container.parent
                    if container: link_tag = container.find('a')
                
                if not link_tag: continue

                link = link_tag.get('href', '')
                title = link_tag.get_text(strip=True)

                if not title: title = link_tag.get('title', '')
                if not link or len(title) < 5: continue

                # X·ª≠ l√Ω Link t∆∞∆°ng ƒë·ªëi
                if link.startswith('/'): 
                    link = "https://ir.datxanh.vn" + link

                # --- PH√ÇN LO·∫†I NGU·ªíN TIN (Source Classification) ---
                # V√¨ DXG g·ªôp chung 1 link, ta ph√¢n lo·∫°i d·ª±a tr√™n Title ƒë·ªÉ d·ªÖ nh√¨n
                title_upper = title.upper()
                source_label = "DXG - C√¥ng b·ªë th√¥ng tin" # M·∫∑c ƒë·ªãnh
                
                if "T√ÄI CH√çNH" in title_upper or "BCTC" in title_upper or "KI·ªÇM TO√ÅN" in title_upper:
                    source_label = "DXG - B√°o c√°o t√†i ch√≠nh"
                elif "ƒê·∫†I H·ªòI" in title_upper or "C·ªî ƒê√îNG" in title_upper or "NGH·ªä QUY·∫æT" in title_upper:
                    source_label = "DXG - ƒêHƒêCƒê"

                # Check tr√πng
                item_id = link
                if item_id in seen_ids: continue

                # L·ªçc nƒÉm
                if str(current_year) not in clean_date: continue

                new_items.append({
                    "source": source_label,
                    "id": item_id,
                    "title": title,
                    "date": clean_date,
                    "link": link
                })
                seen_ids.add(item_id)

            except Exception:
                continue

    except Exception as e:
        print(f"   ! L·ªói khi qu√©t DXG: {e}")

    return new_items

# ==============================================================================
# 16. TCH - T√†i ch√≠nh Ho√†ng Huy (UPDATE V2 - Fix l·ªói s√≥t tin)
# ==============================================================================
# V·∫•n ƒë·ªÅ c≈©: Ch·ªâ b·∫Øt ƒë∆∞·ª£c BCTC (c√≥ ng√†y th√°ng), b·ªè s√≥t ƒêHƒêCƒê/HƒêQT (th∆∞·ªùng ko c√≥ ng√†y).
# Gi·∫£i ph√°p: Th√™m c∆° ch·∫ø qu√©t theo Ti√™u ƒë·ªÅ ch·ª©a "2025".
# ==============================================================================

def fetch_tch_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t TCH (NƒÉm {current_year}) ---")
    
    base_url = "https://www.hoanghuy.vn"
    targets = [
        ("TCH - ƒêHƒêCƒê", "https://www.hoanghuy.vn/dai-hoi-co-dong/"),
        ("TCH - HƒêQT", "https://www.hoanghuy.vn/hoat-dong-cua-hoi-dong-quan-tri/"),
        ("TCH - BCTC", "https://www.hoanghuy.vn/bao-cao-tai-chinh/")
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T 1: T√åM THEO NG√ÄY (Cho BCTC) ---
            # (Gi·ªØ nguy√™n logic c≈© v√¨ n√≥ ƒëang ho·∫°t ƒë·ªông t·ªët cho BCTC)
            date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
            found_ids_pass1 = set()

            for node in date_nodes:
                try:
                    date_str = node.strip()
                    match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                    if not match: continue
                    clean_date = match.group(1)

                    container = node.find_parent(['li', 'tr', 'div', 'p'])
                    if not container: continue

                    link_tag = container.find('a')
                    if not link_tag:
                         container = container.parent
                         if container: link_tag = container.find('a')
                    
                    if not link_tag: continue

                    link = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)
                    if not title: title = link_tag.get('title', '')
                    
                    if not link: continue
                    if link.startswith('/'): link = base_url + link
                    
                    # Validate
                    if len(title) < 5: continue
                    if str(current_year) not in clean_date: continue

                    item_id = link
                    if item_id in seen_ids: continue

                    new_items.append({
                        "source": source_label,
                        "id": item_id,
                        "title": title,
                        "date": clean_date,
                        "link": link
                    })
                    seen_ids.add(item_id)
                    found_ids_pass1.add(item_id)

                except Exception:
                    continue
            
            # --- CHI·∫æN THU·∫¨T 2: QU√âT TI√äU ƒê·ªÄ (Cho ƒêHƒêCƒê v√† HƒêQT) ---
            # N·∫øu item ch∆∞a ƒë∆∞·ª£c l·∫•y ·ªü pass 1, ta qu√©t ti·∫øp d·ª±a tr√™n Title ch·ª©a nƒÉm
            all_links = soup.find_all('a')
            for a in all_links:
                link = a.get('href', '')
                title = a.get_text(strip=True)
                
                if not title: title = a.get('title', '')
                if not link or len(title) < 5: continue
                
                # Chu·∫©n h√≥a link
                if link.startswith('/'): link = base_url + link
                
                # B·ªè qua n·∫øu ƒë√£ l·∫•y ·ªü pass 1 ho·∫∑c ƒë√£ seen
                if link in found_ids_pass1 or link in seen_ids: continue
                
                # ƒêI·ªÄU KI·ªÜN L·∫§Y: Title ph·∫£i ch·ª©a nƒÉm hi·ªán t·∫°i (2025)
                # (D√†nh cho c√°c m·ª•c ko c√≥ ng√†y th√°ng c·ª• th·ªÉ, th∆∞·ªùng ti√™u ƒë·ªÅ s·∫Ω ghi "NƒÉm 2025")
                if str(current_year) in title:
                    # Ng√†y gi·∫£ l·∫≠p (v√¨ web kh√¥ng hi·ªán ng√†y)
                    fake_date = f"01/01/{current_year}"
                    
                    new_items.append({
                        "source": source_label,
                        "id": link,
                        "title": title,
                        "date": fake_date, # Date gi·∫£ ƒë·ªÉ bot kh√¥ng b√°o l·ªói
                        "link": link
                    })
                    seen_ids.add(link)

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_dcm_news(seen_ids):
    """
    H√†m c√†o ƒê·∫°m C√† Mau (DCM).
    - S·ª≠ d·ª•ng Selenium ƒë·ªÉ x·ª≠ l√Ω AJAX (box-document-ajax).
    - Qu√©t 2 m·ª•c: BCTC v√† ƒêHƒêCƒê d·ª±a tr√™n c·∫•u tr√∫c HTML b·∫°n cung c·∫•p.
    """
    
    current_year = datetime.now().year
    url = "https://www.pvcfc.com.vn/quan-he-dau-tu"
    
    # --- C·∫§U H√åNH SELENIUM ---
    chrome_options = Options()
    chrome_options.add_argument("--headless") # Ch·∫°y ng·∫ßm
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    new_items = []
    
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t DCM (NƒÉm {current_year}) ---")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    try:
        driver.get(url)
        
        # 1. Ch·ªù AJAX load d·ªØ li·ªáu (Quan tr·ªçng)
        # Web n√†y load t·ª´ng box, n√™n ta ch·ªù kho·∫£ng 5-7 gi√¢y cho ch·∫Øc
        time.sleep(7)
        
        # L·∫•y to√†n b·ªô HTML ƒë√£ render
        html_content = driver.page_source
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 2. T√¨m c√°c Box t√†i li·ªáu (D·ª±a tr√™n ·∫£nh 1 v√† 3)
        # Class chung l√† "box-document"
        document_boxes = soup.select('.box-document')
        
        for box in document_boxes:
            # L·∫•y ti√™u ƒë·ªÅ Box ƒë·ªÉ ph√¢n lo·∫°i (BCTC hay ƒêHƒêCƒê)
            # Selector: .title.uppercase (·∫¢nh 1)
            title_div = box.select_one('.title.uppercase')
            if not title_div: continue
            
            box_title_text = title_div.get_text(strip=True).lower()
            
            # X√°c ƒë·ªãnh lo·∫°i tin
            category = None
            if "b√°o c√°o t√†i ch√≠nh" in box_title_text:
                category = "BCTC"
            elif "ƒë·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng" in box_title_text:
                category = "ƒêHƒêCƒê"
            
            # N·∫øu kh√¥ng ph·∫£i 2 m·ª•c c·∫ßn t√¨m th√¨ b·ªè qua
            if not category: continue
            
            # 3. Qu√©t c√°c item b√™n trong Box n√†y (·∫¢nh 2, 4, 5)
            # Selector: .document-item
            items = box.select('.document-item')
            
            count_in_box = 0
            for item in items:
                # --- A. L·∫•y Link & Title ---
                # D·ª±a tr√™n ·∫£nh 2: <a class="download" href="..." title="...">
                a_tag = item.select_one('a.download')
                if not a_tag: continue
                
                link = a_tag.get('href')
                # L·∫•y title t·ª´ thu·ªôc t√≠nh title c·ªßa th·∫ª a, n·∫øu kh√¥ng c√≥ th√¨ l·∫•y text b√™n trong div title
                title = a_tag.get('title')
                
                if not title:
                    # Fallback: L·∫•y t·ª´ div.doc-title (·∫¢nh 2)
                    doc_title_div = item.select_one('.doc-title')
                    if doc_title_div: title = doc_title_div.get_text(strip=True)
                
                if not link or not title: continue
                
                # Chu·∫©n h√≥a link
                if not link.startswith('http'):
                    link = f"https://www.pvcfc.com.vn{link}"
                
                # --- B. L·∫•y Ng√†y th√°ng ---
                # D·ª±a tr√™n ·∫£nh 2: <time ...>Th·ª© ba, 28/10/2025</time>
                time_tag = item.select_one('time')
                date_str = str(current_year) # M·∫∑c ƒë·ªãnh
                
                if time_tag:
                    raw_date = time_tag.get_text(strip=True)
                    # X·ª≠ l√Ω chu·ªói "Th·ª© ba, 28/10/2025" -> L·∫•y "28/10/2025"
                    # Logic: T√¨m chu·ªói ng√†y/th√°ng/nƒÉm
                    match = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', raw_date)
                    if match:
                        clean_date_str = match.group(1)
                        try:
                            pub_date = datetime.strptime(clean_date_str, "%d/%m/%Y")
                            
                            # L·ªåC NƒÇM: Ch·ªâ l·∫•y tin nƒÉm nay
                            if pub_date.year != current_year:
                                continue
                            
                            date_str = clean_date_str
                        except:
                            pass
                
                # --- C. L∆∞u tr·ªØ ---
                news_id = link
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue
                
                new_items.append({
                    "source": f"DCM - {category}",
                    "id": news_id,
                    "title": title,
                    "date": date_str,
                    "link": link
                })
                count_in_box += 1
            
            # print(f"   > {category}: T√¨m th·∫•y {count_in_box} tin.")

    except Exception as e:
        print(f"[DCM] L·ªói Selenium: {e}")
    finally:
        driver.quit()
        
    return new_items

def fetch_vpi_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VPI (NƒÉm {current_year}) ---")
    
    url = "https://vanphu.vn/quan-he-co-dong/"
    
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    try:
        resp = session.get(url, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(resp.content, 'html.parser')

        # --- CHI·∫æN THU·∫¨T 1: T√åM THEO NG√ÄY (∆Øu ti√™n) ---
        # T√¨m c√°c node text ch·ª©a ng√†y th√°ng dd/mm/yyyy
        date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
        
        # Danh s√°ch item ƒë√£ t√¨m th·∫•y ·ªü b∆∞·ªõc 1 (ƒë·ªÉ b∆∞·ªõc 2 kh√¥ng l·∫•y tr√πng)
        found_in_pass1 = set()

        for node in date_nodes:
            try:
                date_str = node.strip()
                match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                if not match: continue
                clean_date = match.group(1)

                # T·ª´ ng√†y, t√¨m ra container (th·∫ª bao)
                container = node.find_parent(['div', 'li', 'tr', 'article', 'td'])
                if not container: continue

                # T√¨m link
                link_tag = container.find('a')
                
                # N·∫øu kh√¥ng th·∫•y link, th·ª≠ nh·∫£y l√™n c·∫•p cha
                if not link_tag:
                    container = container.parent
                    if container: link_tag = container.find('a')
                
                if not link_tag: continue

                link = link_tag.get('href', '')
                title = link_tag.get_text(strip=True)
                
                # Fallback title
                if not title: title = link_tag.get('title', '')
                
                if not link or len(title) < 5: continue

                # X·ª≠ l√Ω link
                if link.startswith('/'): 
                    link = "https://vanphu.vn" + link

                # L·ªçc nƒÉm hi·ªán t·∫°i
                if str(current_year) not in clean_date: continue

                # Check tr√πng ID
                item_id = link
                if item_id in seen_ids: continue

                # --- PH√ÇN LO·∫†I TIN (Auto-Tagging) ---
                t_upper = title.upper()
                source_label = "VPI - Tin c·ªï ƒë√¥ng" # M·∫∑c ƒë·ªãnh
                
                if "BCTC" in t_upper or "T√ÄI CH√çNH" in t_upper or "KI·ªÇM TO√ÅN" in t_upper:
                    source_label = "VPI - B√°o c√°o t√†i ch√≠nh"
                elif "ƒêHƒêCƒê" in t_upper or "ƒê·∫†I H·ªòI" in t_upper or "NGH·ªä QUY·∫æT" in t_upper:
                    source_label = "VPI - ƒêHƒêCƒê/HƒêQT"
                elif "QU·∫¢N TR·ªä" in t_upper or "B√ÅO C√ÅO TH∆Ø·ªúNG NI√äN" in t_upper:
                    source_label = "VPI - B√°o c√°o qu·∫£n tr·ªã/TN"

                new_items.append({
                    "source": source_label,
                    "id": item_id,
                    "title": title,
                    "date": clean_date,
                    "link": link
                })
                seen_ids.add(item_id)
                found_in_pass1.add(item_id)

            except Exception:
                continue

        # --- CHI·∫æN THU·∫¨T 2: T√åM THEO TITLE CH·ª®A NƒÇM (Backup) ---
        # D√†nh cho c√°c m·ª•c kh√¥ng hi·ªán ng√†y ra ngo√†i (v√≠ d·ª• B√°o c√°o th∆∞·ªùng ni√™n ch·ªâ ghi t√™n)
        all_links = soup.find_all('a')
        for a in all_links:
            link = a.get('href', '')
            title = a.get_text(strip=True)
            if not title: title = a.get('title', '')
            
            if not link or len(title) < 5: continue
            
            # N·∫øu item n√†y ƒë√£ l·∫•y ·ªü b∆∞·ªõc 1 r·ªìi th√¨ b·ªè qua
            if link.startswith('/'): full_link = "https://vanphu.vn" + link
            else: full_link = link
            
            if full_link in found_in_pass1 or full_link in seen_ids: continue

            # ƒêi·ªÅu ki·ªán: Title ph·∫£i ch·ª©a "2025"
            if str(current_year) in title:
                # Ph√¢n lo·∫°i l·∫°i
                t_upper = title.upper()
                source_label = "VPI - Tin kh√°c"
                if "BCTC" in t_upper or "T√ÄI CH√çNH" in t_upper: source_label = "VPI - BCTC"
                elif "ƒêHƒêCƒê" in t_upper or "NGH·ªä QUY·∫æT" in t_upper: source_label = "VPI - ƒêHƒêCƒê"
                
                fake_date = f"01/01/{current_year}"

                new_items.append({
                    "source": source_label,
                    "id": full_link,
                    "title": title,
                    "date": fake_date,
                    "link": full_link
                })
                seen_ids.add(full_link)

    except Exception as e:
        print(f"   ! L·ªói khi qu√©t VPI: {e}")

    return new_items

def fetch_sjs_news(seen_ids):
    """
    H√†m c√†o SJ Group (SJS).
    - S·ª≠ d·ª•ng Requests (v√¨ d·ªØ li·ªáu c√≥ trong view-source).
    - L·∫•y link t·ª´ thu·ªôc t√≠nh 'data' c·ªßa div.show-data.
    - L·ªçc: Ch·ªâ l·∫•y Ti·∫øng Vi·ªát, b·ªè Ti·∫øng Anh.
    """
    
    current_year = datetime.now().year
    
    # C·∫•u h√¨nh 2 link g·ªëc
    categories = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://sjgroups.com.vn/bao-cao-tai-chinh-fd143.html"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://sjgroups.com.vn/tai-lieu-dai-hoi-dong-co-dong-fd144.html"
        }
    ]
    
    base_domain = "https://sjgroups.com.vn"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # Setup session
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SJS (NƒÉm {current_year}) ---")

    for cat in categories:
        # Qu√©t 3 trang ƒë·∫ßu m·ªói m·ª•c (th∆∞·ªùng l√† ƒë·ªß cho 1 nƒÉm)
        for page in range(1, 4):
            params = {
                "publicdate_time": current_year, # L·ªçc theo nƒÉm 2025
                "page": page
            }
            
            try:
                response = session.get(cat['url'], headers=headers, params=params, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[SJS] L·ªói k·∫øt n·ªëi {cat['name']}: {response.status_code}")
                    break

                soup = BeautifulSoup(response.text, 'html.parser')
                
                # T√¨m b·∫£ng d·ªØ li·ªáu
                # C·∫•u tr√∫c: table -> tbody -> tr (class odd/even)
                rows = soup.select('table tbody tr')
                
                if not rows:
                    # N·∫øu kh√¥ng c√≥ d√≤ng n√†o -> H·∫øt trang ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu
                    break
                
                count_in_page = 0
                
                for row in rows:
                    # 1. T√åM TITLE & LINK (C·ªôt 1 - class="first")
                    first_td = row.select_one('td.first')
                    if not first_td: continue
                    
                    # D·ªØ li·ªáu n·∫±m trong div class="show-data"
                    data_div = first_td.select_one('.show-data')
                    if not data_div: continue
                    
                    # L·∫•y Title
                    title = data_div.get_text(strip=True)
                    
                    # --- L·ªåC NG√îN NG·ªÆ ---
                    # Ch·ªâ l·∫•y Ti·∫øng Vi·ªát -> B·ªè Ti·∫øng Anh
                    if "ti·∫øng anh" in title.lower() or "english" in title.lower():
                        continue
                        
                    # L·∫•y Link t·ª´ thu·ªôc t√≠nh 'data' (ƒê√¢y l√† ch√¨a kh√≥a!)
                    # data="/download-file.html?id=..."
                    relative_link = data_div.get('data')
                    
                    if not relative_link: continue
                    
                    full_link = f"{base_domain}{relative_link}"
                    
                    # 2. T√åM NG√ÄY TH√ÅNG (C·ªôt 2 - class="released")
                    # D·ª±a v√†o ·∫£nh 44d072.png, c·ªôt ng√†y c√≥ id/class li√™n quan publicdate
                    # Nh∆∞ng inspect code trong ·∫£nh th·∫•y: <td class="released">23-10-2025</td> (ƒëo√°n class d·ª±a tr√™n th√≥i quen code table)
                    # N·∫øu soi k·ªπ ·∫£nh 4: C·ªôt ng√†y n·∫±m ngay sau c·ªôt title.
                    # Ta l·∫•y danh s√°ch td, ng√†y th∆∞·ªùng l√† td th·ª© 2 (index 1)
                    tds = row.find_all('td')
                    date_str = str(current_year)
                    if len(tds) >= 2:
                        raw_date = tds[1].get_text(strip=True) # VD: 23-10-2025
                        try:
                            # Parse ng√†y
                            datetime.strptime(raw_date, "%d-%m-%Y")
                            date_str = raw_date
                        except:
                            pass # N·∫øu l·ªói th√¨ gi·ªØ nguy√™n current_year

                    # 3. Check tr√πng & L∆∞u
                    news_id = full_link
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"SJS - {cat['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": full_link
                    })
                    count_in_page += 1
                
                # N·∫øu trang n√†y kh√¥ng c√≥ tin n√†o (sau khi l·ªçc ti·∫øng Anh) -> C√≥ th·ªÉ v·∫´n c√≤n tin ti·∫øng Vi·ªát ·ªü trang sau?
                # Nh∆∞ng n·∫øu rows r·ªóng th√¨ break. N·∫øu rows c√≥ m√† filtered h·∫øt th√¨ c·ª© ch·∫°y ti·∫øp trang sau cho ch·∫Øc.
                if len(rows) == 0:
                    break
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[SJS] L·ªói t·∫°i {cat['name']}: {e}")
                break
                
    return new_items

def fetch_nlg_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t NLG (NƒÉm {current_year}) ---")
    
    url = "https://www.namlongvn.com/quan-he-nha-dau-tu/"
    
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    try:
        resp = session.get(url, headers=headers, timeout=30, verify=False)
        soup = BeautifulSoup(resp.content, 'html.parser')

        # --- CHI·∫æN THU·∫¨T: QU√âT T·∫§T C·∫¢ TH·∫∫ A & L·ªåC THEO URL KEYWORD ---
        all_links = soup.find_all('a', href=True)
        
        # C√°c t·ª´ kh√≥a b·∫Øt bu·ªôc ph·∫£i c√≥ trong Link (ho·∫∑c Title)
        # Nh√≥m 1: BCTC (nh∆∞ ·∫£nh 1,2,3)
        kw_bctc = ['bao-cao-tai-chinh', 'bctc', 'financial-report']
        # Nh√≥m 2: ƒêHƒêCƒê & C·ªï t·ª©c (nh∆∞ ·∫£nh 4,5 v√† y√™u c·∫ßu c·ªßa b·∫°n)
        kw_dhcd = ['dai-hoi-dong-co-dong', 'lay-y-kien', 'co-tuc']

        for a in all_links:
            try:
                link = a['href']
                title = a.get_text(strip=True)
                if not title: title = a.get('title', '')
                
                # Validate c∆° b·∫£n
                if not link or len(title) < 5: continue
                
                # Chu·∫©n h√≥a link
                if link.startswith('/'): 
                    link = "https://www.namlongvn.com" + link
                
                link_lower = link.lower()
                title_lower = title.lower()

                # --- B∆Ø·ªöC 1: PH√ÇN LO·∫†I & L·ªåC ---
                is_bctc = any(k in link_lower for k in kw_bctc)
                is_dhcd = any(k in link_lower for k in kw_dhcd)

                # N·∫øu kh√¥ng thu·ªôc 2 nh√≥m n√†y -> B·ªé QUA (theo y√™u c·∫ßu ch·ªâ l·∫•y ƒë√∫ng lo·∫°i)
                if not is_bctc and not is_dhcd:
                    continue

                # G√°n nh√£n Source
                source_label = "NLG - Tin t·ª©c"
                if is_bctc: source_label = "NLG - B√°o c√°o t√†i ch√≠nh"
                elif is_dhcd: source_label = "NLG - ƒêHƒêCƒê/C·ªï t·ª©c"

                # --- B∆Ø·ªöC 2: T√åM NG√ÄY TH√ÅNG ---
                # T√¨m ng√†y trong ch√≠nh th·∫ª a ho·∫∑c th·∫ª cha c·ªßa n√≥
                date_str = ""
                
                # Th·ª≠ t√¨m trong th·∫ª cha (div/li/tr)
                container = a.find_parent(['div', 'li', 'tr', 'article'])
                if container:
                    txt = container.get_text(" ", strip=True)
                    match = re.search(r"(\d{2}/\d{2}/\d{4})", txt)
                    if match: date_str = match.group(1)
                
                # N·∫øu kh√¥ng th·∫•y ng√†y, nh∆∞ng Title c√≥ ch·ª©a NƒÉm hi·ªán t·∫°i -> L·∫•y lu√¥n
                if not date_str:
                    if str(current_year) in title:
                        date_str = f"01/01/{current_year}"

                # --- B∆Ø·ªöC 3: CHECK ---
                if not date_str: continue # Kh√¥ng c√≥ ng√†y -> B·ªè
                
                # L·ªçc nƒÉm 2025
                if str(current_year) not in date_str and str(current_year) not in title:
                    continue

                if link in seen_ids: continue

                new_items.append({
                    "source": source_label,
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })
                seen_ids.add(link)

            except Exception:
                continue

    except Exception as e:
        print(f"   ! L·ªói khi qu√©t NLG: {e}")

    return new_items

def fetch_pvs_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t PVS (NƒÉm {current_year}) ---")
    
    # URL m·ª•c BCTC
    url = "https://www.ptsc.com.vn/co-dong/danh-cho-co-dong/bao-cao-tai-chinh"
    
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    try:
        resp = session.get(url, headers=headers, timeout=30, verify=False)
        soup = BeautifulSoup(resp.content, 'html.parser')

        # --- CHI·∫æN THU·∫¨T 1: T√åM THEO NG√ÄY (∆Øu ti√™n) ---
        # T√¨m text ch·ª©a ng√†y th√°ng d·∫°ng dd/mm/yyyy
        date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
        
        found_ids_pass1 = set()

        for node in date_nodes:
            try:
                date_str = node.strip()
                match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                if not match: continue
                clean_date = match.group(1)

                # T√¨m th·∫ª bao (container) ch·ª©a link
                # PVS th∆∞·ªùng d√πng div class 'item' ho·∫∑c tr
                container = node.find_parent(['div', 'li', 'tr', 'article'])
                if not container: continue

                # T√¨m Link & Title
                link_tag = container.find('a')
                
                # N·∫øu kh√¥ng th·∫•y link ngay c·∫°nh ng√†y, th·ª≠ t√¨m trong th·∫ª cha c·ªßa container
                if not link_tag:
                     container = container.parent
                     if container: link_tag = container.find('a')
                
                if not link_tag: continue

                link = link_tag.get('href', '')
                title = link_tag.get_text(strip=True)
                if not title: title = link_tag.get('title', '')

                if not link or len(title) < 5: continue
                
                # Chu·∫©n h√≥a Link
                if link.startswith('/'): 
                    link = "https://www.ptsc.com.vn" + link

                # L·ªçc nƒÉm 2025
                if str(current_year) not in clean_date: continue

                # Check tr√πng
                item_id = link
                if item_id in seen_ids: continue

                new_items.append({
                    "source": "PVS - B√°o c√°o t√†i ch√≠nh",
                    "id": item_id,
                    "title": title,
                    "date": clean_date,
                    "link": link
                })
                seen_ids.add(item_id)
                found_ids_pass1.add(item_id)

            except Exception:
                continue

        # --- CHI·∫æN THU·∫¨T 2: QU√âT THEO TITLE CH·ª®A NƒÇM (Backup) ---
        # N·∫øu web PVS ·∫©n ng√†y th√°ng, ta qu√©t c√°c link c√≥ ch·ª©a "2025" trong title
        all_links = soup.find_all('a')
        for a in all_links:
            link = a.get('href', '')
            title = a.get_text(strip=True)
            if not title: title = a.get('title', '')
            
            if not link or len(title) < 5: continue
            
            # Chu·∫©n h√≥a link
            if link.startswith('/'): link = "https://www.ptsc.com.vn" + link
            
            # B·ªè qua n·∫øu ƒë√£ l·∫•y ·ªü pass 1 ho·∫∑c ƒë√£ seen
            if link in found_ids_pass1 or link in seen_ids: continue
            
            # ƒêi·ªÅu ki·ªán: Title ph·∫£i ch·ª©a "2025"
            if str(current_year) in title:
                new_items.append({
                    "source": "PVS - B√°o c√°o t√†i ch√≠nh",
                    "id": link,
                    "title": title,
                    "date": f"01/01/{current_year}", # Gi·∫£ l·∫≠p ng√†y
                    "link": link
                })
                seen_ids.add(link)

    except Exception as e:
        print(f"   ! L·ªói khi qu√©t PVS: {e}")

    return new_items

def fetch_tal_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t TAL (NƒÉm {current_year}) ---")
    
    base_url = "https://tasecoland.vn"
    targets = [
        ("TAL - ƒêHƒêCƒê", "https://tasecoland.vn/dai-hoi-dong-co-dong-nam-2025-34251157"),
        ("TAL - ƒêHƒêCƒê (T√†i li·ªáu)", "https://tasecoland.vn/dai-hoi-dong-co-dong-nam-2025-34251157?tailieu=2"),
        ("TAL - B√°o c√°o t√†i ch√≠nh", "https://tasecoland.vn/bao-cao-tai-chinh-nam-2025-34251249")
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=30, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T: V√âT C·∫†N LINK (Safe Scan) ---
            # T√¨m t·∫•t c·∫£ th·∫ª a, sau ƒë√≥ l·ªçc k·ªπ
            all_links = soup.find_all('a', href=True)

            for a in all_links:
                try:
                    link = a['href']
                    title = a.get_text(strip=True)
                    # N·∫øu title r·ªóng, l·∫•y attribute title
                    if not title: title = a.get('title', '')
                    
                    if not link or len(title) < 5: continue

                    # --- 1. L·ªåC NG√îN NG·ªÆ (VIETNAMESE ONLY) ---
                    title_lower = title.lower()
                    link_lower = link.lower()
                    
                    # B·ªè qua n·∫øu l√† ti·∫øng Anh
                    if "english" in title_lower or "(en)" in title_lower or "_en" in link_lower:
                        continue

                    # --- 2. X·ª¨ L√ù NG√ÄY TH√ÅNG ---
                    date_str = ""
                    
                    # C√°ch 1: T√¨m ng√†y trong text c·ªßa th·∫ª cha (div/li/tr/td)
                    # TAL th∆∞·ªùng ƒë·ªÉ ng√†y trong 1 th·∫ª span ho·∫∑c td b√™n c·∫°nh
                    container = a.find_parent(['tr', 'li', 'div', 'p'])
                    if container:
                        txt = container.get_text(" ", strip=True)
                        match = re.search(r"(\d{2}/\d{2}/\d{4})", txt)
                        if match: date_str = match.group(1)

                    # C√°ch 2: Backup - N·∫øu title ch·ª©a nƒÉm 2025 -> L·∫•y
                    if not date_str:
                        if str(current_year) in title:
                            date_str = f"01/01/{current_year}"

                    if not date_str: continue

                    # --- 3. CHECK H·ª¢P L·ªÜ ---
                    # Chu·∫©n h√≥a link
                    if link.startswith('/'): 
                        link = base_url + link
                    
                    # L·ªçc nƒÉm 2025
                    if str(current_year) not in date_str and str(current_year) not in title:
                        continue

                    if link in seen_ids: continue

                    new_items.append({
                        "source": source_label,
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    seen_ids.add(link)

                except Exception:
                    continue

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_qns_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t QNS (NƒÉm {current_year}) ---")
    
    base_url = "https://qns.com.vn"
    targets = [
        ("QNS - ƒêHƒêCƒê", "https://qns.com.vn/dai-hoi-co-dong"),
        ("QNS - B√°o c√°o t√†i ch√≠nh", "https://qns.com.vn/bao-cao-tai-chinh")
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=30, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T 1: T√åM THEO NG√ÄY (∆Øu ti√™n) ---
            # T√¨m text ch·ª©a ng√†y th√°ng d·∫°ng dd/mm/yyyy ho·∫∑c dd-mm-yyyy
            date_nodes = soup.find_all(string=re.compile(r'\d{2}[/-]\d{2}[/-]\d{4}'))
            
            found_ids_pass1 = set()

            for node in date_nodes:
                try:
                    date_str = node.strip()
                    match = re.search(r"(\d{2}[/-]\d{2}[/-]\d{4})", date_str)
                    if not match: continue
                    clean_date = match.group(1).replace('-', '/') # Chu·∫©n h√≥a v·ªÅ /

                    # T√¨m th·∫ª bao (container) ch·ª©a link
                    # QNS th∆∞·ªùng d√πng th·∫ª div class 'item' ho·∫∑c tr/li
                    container = node.find_parent(['div', 'li', 'tr', 'article', 'td'])
                    if not container: continue

                    # T√¨m Link
                    link_tag = container.find('a')
                    
                    # N·∫øu kh√¥ng th·∫•y link ngay c·∫°nh ng√†y, th·ª≠ t√¨m trong th·∫ª cha c·ªßa container (leo l√™n 1 c·∫•p)
                    if not link_tag:
                         container = container.parent
                         if container: link_tag = container.find('a')
                    
                    if not link_tag: continue

                    link = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)
                    if not title: title = link_tag.get('title', '')

                    if not link or len(title) < 5: continue
                    
                    # Chu·∫©n h√≥a Link
                    if link.startswith('/'): 
                        link = base_url + link

                    # L·ªçc nƒÉm 2025
                    if str(current_year) not in clean_date: continue

                    # Check tr√πng
                    item_id = link
                    if item_id in seen_ids: continue

                    new_items.append({
                        "source": source_label,
                        "id": item_id,
                        "title": title,
                        "date": clean_date,
                        "link": link
                    })
                    seen_ids.add(item_id)
                    found_ids_pass1.add(item_id)

                except Exception:
                    continue

            # --- CHI·∫æN THU·∫¨T 2: QU√âT THEO TITLE CH·ª®A NƒÇM (Backup) ---
            # N·∫øu web QNS ·∫©n ng√†y th√°ng ·ªü m·ªôt s·ªë m·ª•c, qu√©t title c√≥ "2025"
            all_links = soup.find_all('a')
            for a in all_links:
                link = a.get('href', '')
                title = a.get_text(strip=True)
                if not title: title = a.get('title', '')
                
                if not link or len(title) < 5: continue
                
                # Chu·∫©n h√≥a link
                if link.startswith('/'): link = base_url + link
                
                # B·ªè qua n·∫øu ƒë√£ l·∫•y ·ªü pass 1 ho·∫∑c ƒë√£ seen
                if link in found_ids_pass1 or link in seen_ids: continue
                
                # ƒêi·ªÅu ki·ªán: Title ph·∫£i ch·ª©a "2025"
                if str(current_year) in title:
                    new_items.append({
                        "source": source_label,
                        "id": link,
                        "title": title,
                        "date": f"01/01/{current_year}", # Gi·∫£ l·∫≠p ng√†y
                        "link": link
                    })
                    seen_ids.add(link)

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_dig_news(seen_ids):
    """
    H√†m c√†o DIC Corp (DIG).
    - C·∫•u tr√∫c: HTML tƒ©nh.
    - D·ªØ li·ªáu n·∫±m trong div.intro.intro1
    - Ng√†y th√°ng: <i> n·∫±m trong <span> sau icon calendar.
    """
    
    current_year = datetime.now().year
    base_domain = "https://www.dic.vn"
    
    # C√°c danh m·ª•c c·∫ßn c√†o (d·ª±a tr√™n link b·∫°n g·ª≠i v√† c·∫•u tr√∫c trong ·∫£nh 4 c√≥ th√™m 'cong-bo-thong-tin')
    categories = [
        "dai-hoi-co-dong-thuong-nien",
        "bao-cao-tai-chinh",
        "cong-bo-thong-tin" # Th√™m c√°i n√†y d·ª±a tr√™n ·∫£nh 4 b·∫°n g·ª≠i
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t DIG (NƒÉm {current_year}) ---")

    for cat in categories:
        url = f"{base_domain}/{cat}"
        
        # Trang n√†y th∆∞·ªùng show nhi·ªÅu tin m·ªôt l√∫c, nh∆∞ng ta c·ª© th·ª≠ loop page=1,2 n·∫øu c·∫ßn
        # Tuy nhi√™n link b·∫°n ƒë∆∞a l√† d·∫°ng category root, ta qu√©t trang ƒë·∫ßu tr∆∞·ªõc.
        # N·∫øu web d√πng pagination d·∫°ng ?page=2 ho·∫∑c /page/2, b·∫°n c√≥ th·ªÉ m·ªü r·ªông v√≤ng l·∫∑p.
        # ·ªû ƒë√¢y m√¨nh qu√©t trang ch·ªß c·ªßa danh m·ª•c (th∆∞·ªùng ch·ª©a tin m·ªõi nh·∫•t).
        
        try:
            # print(f"   >> ƒêang t·∫£i: {cat}...")
            response = session.get(url, headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[DIG] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c kh·ªëi tin (·∫¢nh 1, 3: div class="item col-md-6")
            items = soup.select('.item.col-md-6')
            
            count_in_cat = 0
            
            for item in items:
                # T√¨m v√†o kh·ªëi intro1 (·∫¢nh 1, 3)
                intro_div = item.select_one('.intro.intro1')
                if not intro_div: continue
                
                # 1. L·∫•y Ti√™u ƒë·ªÅ & Link
                title_tag = intro_div.select_one('a.title')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                relative_link = title_tag.get('href')
                
                if not relative_link: continue
                
                # X·ª≠ l√Ω Link (Link trong ·∫£nh l√† relative: "bao-cao-tai-chinh/...")
                if not relative_link.startswith('http'):
                    # ƒê·∫£m b·∫£o kh√¥ng b·ªã double slash
                    if relative_link.startswith('/'):
                        full_link = f"{base_domain}{relative_link}"
                    else:
                        full_link = f"{base_domain}/{relative_link}"
                else:
                    full_link = relative_link

                # 2. L·∫•y Ng√†y th√°ng
                # C·∫•u tr√∫c ·∫£nh 1: <i class="fa fa-calendar"></i><span><i> 21/04/2025</i></span>
                # T√¨m th·∫ª i c√≥ class fa-calendar, sau ƒë√≥ t√¨m th·∫ª span k·∫ø ti·∫øp, r·ªìi l·∫•y text b√™n trong
                date_str = str(current_year)
                
                # C√°ch 1: T√¨m theo sibling
                calendar_icon = intro_div.select_one('.fa-calendar')
                if calendar_icon:
                    # T√¨m th·∫ª span ngay sau icon
                    date_span = calendar_icon.find_next_sibling('span')
                    if date_span:
                        raw_date = date_span.get_text(strip=True) # VD: 21/04/2025
                        try:
                            pub_date = datetime.strptime(raw_date, "%d/%m/%Y")
                            
                            # L·ªåC NƒÇM
                            if pub_date.year != current_year:
                                continue # B·ªè qua tin c≈©
                                
                            date_str = raw_date
                        except:
                            pass
                
                # 3. Check tr√πng & L∆∞u
                news_id = full_link
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue

                new_items.append({
                    "source": f"DIG - {cat}",
                    "id": news_id,
                    "title": title,
                    "date": date_str,
                    "link": full_link
                })
                count_in_cat += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[DIG] L·ªói x·ª≠ l√Ω {cat}: {e}")
            continue

    return new_items

def fetch_dpm_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t DPM (NƒÉm {current_year}) ---")
    
    base_url = "https://dpm.vn"
    targets = [
        ("DPM - B√°o c√°o t√†i ch√≠nh", "https://dpm.vn/bao-cao-tai-chinh"),
        ("DPM - C√¥ng b·ªë th√¥ng tin", "https://dpm.vn/cong-bo-thong-tin")
    ]

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    # T·ª´ kh√≥a b·∫Øt bu·ªôc cho m·ª•c C√¥ng b·ªë th√¥ng tin
    keywords_filter = ['dai-hoi-dong-co-dong', 'hoi-dong-quan-tri', 'co-tuc', 'lay-y-kien']

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=30, verify=False)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T: QU√âT T·∫§T C·∫¢ LINK (V√âT C·∫†N & L·ªåC) ---
            # T√¨m t·∫•t c·∫£ th·∫ª a, sau ƒë√≥ l·ªçc k·ªπ
            all_links = soup.find_all('a', href=True)

            for a in all_links:
                try:
                    link = a['href']
                    title = a.get_text(strip=True)
                    if not title: title = a.get('title', '')
                    
                    if not link or len(title) < 5: continue

                    # Chu·∫©n h√≥a link
                    if link.startswith('/'): 
                        link = base_url + link

                    # --- 2. T√åM NG√ÄY TH√ÅNG ---
                    date_str = ""
                    
                    # C√°ch 1: T√¨m ng√†y trong text c·ªßa th·∫ª cha (div/li/tr/td)
                    # DPM th∆∞·ªùng c√≥ th·∫ª <span class="date"> ho·∫∑c t∆∞∆°ng t·ª±
                    container = a.find_parent(['div', 'li', 'tr', 'article'])
                    if container:
                        txt = container.get_text(" ", strip=True)
                        # Regex t√¨m dd/mm/yyyy
                        match = re.search(r"(\d{2}/\d{2}/\d{4})", txt)
                        if match: date_str = match.group(1)

                    # C√°ch 2: N·∫øu kh√¥ng th·∫•y ng√†y, check Title ch·ª©a nƒÉm hi·ªán t·∫°i
                    if not date_str:
                        if str(current_year) in title:
                            date_str = f"01/01/{current_year}"

                    if not date_str: continue

                    # --- 3. CHECK H·ª¢P L·ªÜ ---
                    # L·ªçc nƒÉm 2025
                    if str(current_year) not in date_str and str(current_year) not in title:
                        continue

                    if link in seen_ids: continue

                    new_items.append({
                        "source": source_label,
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    seen_ids.add(link)

                except Exception:
                    continue

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items

def fetch_vcg_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VCG (Selenium - NƒÉm {current_year}) ---")
    
    targets = [
        ("VCG - B√°o c√°o t√†i ch√≠nh", "https://vinaconex.com.vn/quan-he-co-dong/bao-cao-tai-chinh"),
        ("VCG - ƒêHƒêCƒê", "https://vinaconex.com.vn/quan-he-co-dong/dai-hoi-co-dong")
    ]

    new_items = []
    
    # C·∫•u h√¨nh Selenium
    chrome_options = Options()
    chrome_options.add_argument("--headless") # Ch·∫°y ng·∫ßm (n·∫øu mu·ªën xem t·∫≠n m·∫Øt th√¨ comment d√≤ng n√†y)
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled") 
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
    chrome_options.add_argument("--ignore-certificate-errors")

    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(60)
    except Exception as e:
        print(f"[VCG] L·ªói kh·ªüi t·∫°o Driver: {e}")
        return []

    try:
        for source_label, url in targets:
            try:
                # print(f"   >> ƒêang truy c·∫≠p: {source_label}...")
                driver.get(url)
                
                # Ch·ªù n·ªôi dung load (t√¨m th·∫ª body ho·∫∑c v√πng content)
                try:
                    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "a")))
                except:
                    pass
                
                time.sleep(3) # Ch·ªù th√™m ch√∫t cho ch·∫Øc
                
                # L·∫•y HTML v·ªÅ x·ª≠ l√Ω b·∫±ng BeautifulSoup cho nhanh v√† chu·∫©n
                html_source = driver.page_source
                soup = BeautifulSoup(html_source, 'html.parser')

                # --- CHI·∫æN THU·∫¨T: QU√âT T·∫§T C·∫¢ LINK ---
                # T√¨m v√πng n·ªôi dung ch√≠nh ƒë·ªÉ lo·∫°i b·ªè Menu/Footer
                # VCG th∆∞·ªùng ƒë·ªÉ n·ªôi dung trong col-md-9 ho·∫∑c col-lg-9
                content_area = soup.find('div', class_=re.compile(r'(col-md|col-lg|main-content)'))
                if not content_area: content_area = soup # Fallback

                all_links = content_area.find_all('a', href=True)
                
                count_found = 0
                for a in all_links:
                    try:
                        link = a['href']
                        title = a.get_text(strip=True)
                        if not title: title = a.get('title', '')
                        
                        # B·ªè qua link qu√° ng·∫Øn ho·∫∑c r·ªóng
                        if not link or len(title) < 5: continue
                        
                        # --- 1. L·ªåC HEADER (QUAN TR·ªåNG) ---
                        # N·∫øu ti√™u ƒë·ªÅ ch·ªâ ch·ª©a th√¥ng tin chung chung nh∆∞ "Qu√Ω 3/2025", "NƒÉm 2025" -> B·ªè qua
                        t_lower = title.lower()
                        # Regex check xem title c√≥ ph·∫£i ch·ªâ to√†n l√† "Qu√Ω ... NƒÉm ..." kh√¥ng
                        if len(title) < 25 and ("qu√Ω" in t_lower or "nƒÉm" in t_lower or "b√°n ni√™n" in t_lower):
                            # V√≠ d·ª•: "B√°o c√°o t√†i ch√≠nh Qu√Ω 3 nƒÉm 2025" -> C√≥ th·ªÉ l√† header
                            # Nh∆∞ng "Gi·∫£i tr√¨nh BCTC Qu√Ω 3..." -> L√† tin th·∫≠t
                            # C√°ch ƒë∆°n gi·∫£n: Header th∆∞·ªùng kh√¥ng c√≥ ng√†y th√°ng c·ª• th·ªÉ (dd/mm)
                            pass 

                        # --- 2. T√åM NG√ÄY TH√ÅNG ---
                        date_str = ""
                        
                        # T√¨m trong text c·ªßa th·∫ª a ho·∫∑c th·∫ª cha (li, tr, div)
                        container = a.find_parent(['li', 'tr', 'div', 'p'])
                        if container:
                            txt = container.get_text(" ", strip=True)
                            # Regex t√¨m dd/mm/yyyy ho·∫∑c dd.mm.yyyy
                            match = re.search(r"(\d{2}[./-]\d{2}[./-]\d{4})", txt)
                            if match: 
                                date_str = match.group(1).replace('.', '/').replace('-', '/')

                        # Backup: N·∫øu kh√¥ng th·∫•y ng√†y, check xem Title c√≥ nƒÉm hi·ªán t·∫°i kh√¥ng
                        # NH∆ØNG: V·ªõi VCG, n·∫øu d√πng backup n√†y d·ªÖ d√≠nh Header.
                        # N√™n ta ch·ªâ d√πng backup n·∫øu title ƒê·ª¶ D√ÄI (nghƒ©a l√† title vƒÉn b·∫£n th·ª±c s·ª±)
                        if not date_str:
                            if str(current_year) in title and len(title) > 20: 
                                date_str = f"01/01/{current_year}"

                        if not date_str: continue

                        # --- 3. CHECK CU·ªêI ---
                        if str(current_year) not in date_str and str(current_year) not in title:
                            continue

                        # Chu·∫©n h√≥a Link
                        if not link.startswith('http'):
                            if link.startswith('/'):
                                link = "https://vinaconex.com.vn" + link
                            else:
                                link = "https://vinaconex.com.vn/" + link

                        if link in seen_ids: continue

                        new_items.append({
                            "source": source_label,
                            "id": link,
                            "title": title,
                            "date": date_str,
                            "link": link
                        })
                        seen_ids.add(link)
                        count_found += 1

                    except Exception:
                        continue
                
                # print(f"      => T√¨m th·∫•y {count_found} tin.")

            except Exception as e:
                print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    finally:
        driver.quit()

    return new_items

def fetch_idc_news(seen_ids):
    """
    H√†m c√†o IDICO (IDC).
    - L·ªçc c·ª©ng nƒÉm 2025.
    - L·∫•y ng√†y th√°ng t·ª´ Item cha (B√†i vi·∫øt) ƒë·ªÉ ch√≠nh x√°c th·ªùi ƒëi·ªÉm c√¥ng b·ªë.
    - L·ªçc t·ª´ kh√≥a cho m·ª•c CBTT.
    """
    
    # --- C·∫§U H√åNH ---
    TARGET_YEAR = 2025  # <--- G√ÅN C·ª®NG NƒÇM 2025
    base_api_domain = "https://admin.idico.com.vn"
    
    keywords_cbtt = [
        "c·ªï t·ª©c", "b√°o c√°o t√†i ch√≠nh", "bctc", 
        "h·ªôi ƒë·ªìng qu·∫£n tr·ªã", "hƒëqt", "l·∫•y √Ω ki·∫øn"
    ]

    targets = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://admin.idico.com.vn/api/tai-lieus",
            "params": {
                "populate": "files.media",
                "filters[category][$eq]": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
                "filters[files][title][$containsi]": "",
                "locale": "vi",
                "sort[0]": "updatedAt:desc"
            },
            "filter_keywords": False
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://admin.idico.com.vn/api/tai-lieus",
            "params": {
                "populate": "files.media",
                "filters[category][$eq]": "C√¥ng b·ªë th√¥ng tin",
                "filters[files][title][$containsi]": "",
                "locale": "vi",
                "sort[0]": "updatedAt:desc"
            },
            "filter_keywords": True
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Origin": "https://www.idico.com.vn",
        "Referer": "https://www.idico.com.vn/"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t IDC (Ch·ªâ l·∫•y nƒÉm {TARGET_YEAR}) ---")

    for target in targets:
        try:
            response = session.get(target["url"], headers=headers, params=target["params"], timeout=20, verify=False)
            if response.status_code != 200:
                print(f"[IDC] L·ªói API {target['name']}: {response.status_code}")
                continue

            json_data = response.json()
            data_list = json_data.get("data", [])
            
            if not data_list: continue

            count_in_cat = 0
            
            for item in data_list:
                attributes = item.get("attributes", {})
                
                # --- 1. KI·ªÇM TRA NG√ÄY C·ª¶A G√ìI TIN (QUAN TR·ªåNG) ---
                # ∆Øu ti√™n updatedAt c·ªßa b√†i vi·∫øt (Item cha)
                item_date_str = attributes.get("updatedAt") 
                if not item_date_str: 
                    item_date_str = attributes.get("publishedAt") # Fallback
                
                date_display = str(TARGET_YEAR)
                is_valid_year = False
                
                if item_date_str:
                    try:
                        # Format: 2025-07-29T10:13:42.100Z
                        dt_obj = datetime.fromisoformat(item_date_str.replace("Z", "+00:00"))
                        
                        # LOGIC L·ªåC NƒÇM
                        if dt_obj.year == TARGET_YEAR:
                            is_valid_year = True
                            date_display = dt_obj.strftime("%d/%m/%Y")
                    except:
                        pass
                
                # N·∫øu b√†i vi·∫øt kh√¥ng ph·∫£i nƒÉm 2025 -> B·ªè qua
                if not is_valid_year: 
                    continue

                # --- 2. X·ª¨ L√ù FILES ---
                files = attributes.get("files", [])
                # Tr∆∞·ªùng h·ª£p files l√† dict (m·ªôt file) ho·∫∑c list (nhi·ªÅu file)
                if isinstance(files, dict) and "data" in files:
                     # ƒê√¥i khi Strapi tr·∫£ v·ªÅ c·∫•u tr√∫c l·∫°, nh∆∞ng theo ·∫£nh b·∫°n g·ª≠i th√¨ files l√† m·∫£ng tr·ª±c ti·∫øp b√™n trong attributes?
                     # Nh√¨n l·∫°i ·∫£nh 1: attributes -> files -> [ {id:..., title:...} ]
                     # V·∫≠y files l√† list c√°c object file.
                     pass
                
                if not files: continue

                for file_info in files:
                    title = file_info.get("title")
                    if not title: continue

                    # L·ªçc T·ª´ kh√≥a (Ch·ªâ √°p d·ª•ng cho CBTT)
                    if target["filter_keywords"]:
                        lower_title = title.lower()
                        is_match = False
                        for kw in keywords_cbtt:
                            if kw.lower() in lower_title:
                                is_match = True
                                break
                        if not is_match: continue 

                    # L·∫•y Link
                    media_obj = file_info.get("media", {})
                    if not media_obj: continue
                    
                    media_data = media_obj.get("data")
                    if not media_data: continue
                    
                    media_attrs = media_data.get("attributes", {})
                    relative_url = media_attrs.get("url")
                    
                    if not relative_url: continue
                    full_link = f"{base_api_domain}{relative_url}"

                    # Check tr√πng
                    news_id = full_link
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"IDC - {target['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_display,
                        "link": full_link
                    })
                    count_in_cat += 1
            
            # print(f"   > {target['name']}: L·∫•y ƒë∆∞·ª£c {count_in_cat} t√†i li·ªáu.")
            time.sleep(0.5)

        except Exception as e:
            print(f"[IDC] L·ªói x·ª≠ l√Ω {target['name']}: {e}")
            continue

    return new_items

def fetch_abb_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t ABB (NƒÉm {current_year}) ---")
    
    url = "https://abbank.vn/thong-tin/tin-tuc-co-dong"
    
    # C·∫•u h√¨nh M·∫°ng m·∫°nh (Retry + Headers x·ªãn)
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    session.verify = False # B·ªè qua l·ªói SSL (th∆∞·ªùng g·∫∑p v·ªõi web bank VN)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://abbank.vn/'
    }

    new_items = []

    try:
        resp = session.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(resp.content, 'html.parser')

        # --- CHI·∫æN THU·∫¨T: NEO THEO NG√ÄY (Date Anchoring) ---
        # T√¨m t·∫•t c·∫£ c√°c text c√≥ ƒë·ªãnh d·∫°ng ng√†y dd/mm/yyyy
        date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
        
        for node in date_nodes:
            try:
                date_str = node.strip()
                # L·∫•y ch√≠nh x√°c ng√†y (ph√≤ng tr∆∞·ªùng h·ª£p text c√≥ l·∫´n ch·ªØ kh√°c)
                match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                if not match: continue
                clean_date = match.group(1)

                # T√¨m th·∫ª bao (container) ch·ª©a c·∫£ Ng√†y v√† Link
                # Th∆∞·ªùng l√† th·∫ª div, li, tr ho·∫∑c article
                container = node.find_parent(['div', 'li', 'tr', 'article', 'h3', 'p'])
                if not container: continue

                # T·ª´ container, t√¨m Link (th·∫ª a)
                link_tag = container.find('a')
                
                # N·∫øu kh√¥ng th·∫•y link, th·ª≠ leo l√™n 1 c·∫•p cha n·ªØa (tr∆∞·ªùng h·ª£p c·∫•u tr√∫c l·ªìng nhau ph·ª©c t·∫°p)
                if not link_tag:
                    container = container.parent
                    if container: link_tag = container.find('a')
                
                if not link_tag: continue

                link = link_tag.get('href', '')
                title = link_tag.get_text(strip=True)
                
                # N·∫øu title trong th·∫ª a qu√° ng·∫Øn (v√≠ d·ª• "Xem th√™m"), t√¨m title ·ªü th·∫ª kh√°c trong c√πng container
                if len(title) < 5:
                    title_tag = container.find(['h2', 'h3', 'h4', 'span'], class_=re.compile(r'(title|name)'))
                    if title_tag: title = title_tag.get_text(strip=True)
                    # N·∫øu v·∫´n kh√¥ng c√≥, l·∫•y attribute title c·ªßa th·∫ª a
                    if not title: title = link_tag.get('title', '')

                if not link or len(title) < 5: continue

                # Chu·∫©n h√≥a Link (ABB hay d√πng link t∆∞∆°ng ƒë·ªëi)
                if link.startswith('/'):
                    link = "https://abbank.vn" + link

                # L·ªçc NƒÉm (Ch·ªâ l·∫•y 2025)
                if str(current_year) not in clean_date: continue

                # Check tr√πng
                item_id = link
                if item_id in seen_ids: continue

                new_items.append({
                    "source": "ABB - Tin c·ªï ƒë√¥ng",
                    "id": item_id,
                    "title": title,
                    "date": clean_date,
                    "link": link
                })
                seen_ids.add(item_id)

            except Exception:
                continue
                
        # --- CHI·∫æN THU·∫¨T PH·ª§: QU√âT LINK CH·ª®A NƒÇM (Backup) ---
        # N·∫øu web ·∫©n ng√†y, qu√©t c√°c link c√≥ title ch·ª©a "2025"
        if not new_items:
            # print("   ‚ö†Ô∏è Kh√¥ng th·∫•y ng√†y, qu√©t backup theo Title...")
            all_links = soup.find_all('a', href=True)
            for a in all_links:
                link = a['href']
                title = a.get_text(strip=True)
                if not title: title = a.get('title', '')
                
                if len(title) > 10 and str(current_year) in title:
                    if link.startswith('/'): link = "https://abbank.vn" + link
                    if link in seen_ids: continue
                    
                    new_items.append({
                        "source": "ABB - Tin c·ªï ƒë√¥ng",
                        "id": link,
                        "title": title,
                        "date": f"01/01/{current_year}", # Ng√†y gi·∫£ ƒë·ªãnh
                        "link": link
                    })
                    seen_ids.add(link)

    except Exception as e:
        print(f"   ! L·ªói khi qu√©t ABB: {e}")

    return new_items

def fetch_pvd_news(seen_ids):
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t PVD (NƒÉm {current_year}) ---")
    
    base_url = "https://www.pvdrilling.com.vn"
    targets = [
        ("PVD - ƒêHƒêCƒê (Tin t·ª©c)", "https://www.pvdrilling.com.vn/quan-he-co-dong/dai-hoi-dong-co-dong"),
        ("PVD - ƒêHƒêCƒê (T√†i li·ªáu)", "https://www.pvdrilling.com.vn/quan-he-co-dong/tai-lieu-dhdcd"),
        ("PVD - B√°o c√°o t√†i ch√≠nh", "https://www.pvdrilling.com.vn/quan-he-co-dong/bao-cao-tai-chinh")
    ]
    
    # C·∫•u h√¨nh Session m·∫°nh
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    session.verify = False
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.pvdrilling.com.vn/'
    }

    new_items = []

    for source_label, url in targets:
        try:
            resp = session.get(url, headers=headers, timeout=30)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # --- CHI·∫æN THU·∫¨T: T√åM NG√ÄY -> SUY RA LINK ---
            # T√¨m t·∫•t c·∫£ node text ch·ª©a ng√†y dd/mm/yyyy
            date_nodes = soup.find_all(string=re.compile(r'\d{2}/\d{2}/\d{4}'))
            
            # Bi·∫øn c·ªù ƒë·ªÉ bi·∫øt trang n√†y c√≥ t√¨m ƒë∆∞·ª£c tin n√†o kh√¥ng
            found_any_in_page = False

            for node in date_nodes:
                try:
                    date_str = node.strip()
                    match = re.search(r"(\d{2}/\d{2}/\d{4})", date_str)
                    if not match: continue
                    clean_date = match.group(1)

                    # T√¨m Container (th·∫ª bao)
                    container = node.find_parent(['div', 'li', 'tr', 'article', 'td'])
                    if not container: continue

                    # T√¨m Link
                    link_tag = container.find('a')
                    # N·∫øu kh√¥ng th·∫•y, leo l√™n 1 c·∫•p n·ªØa
                    if not link_tag:
                        container = container.parent
                        if container: link_tag = container.find('a')
                    
                    if not link_tag: continue

                    link = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)
                    if not title: title = link_tag.get('title', '')

                    if not link or len(title) < 5: continue

                    # Chu·∫©n h√≥a Link
                    if link.startswith('/'):
                        link = base_url + link
                    
                    # L·ªçc NƒÉm
                    if str(current_year) not in clean_date: continue

                    # Check tr√πng
                    if link in seen_ids: continue

                    new_items.append({
                        "source": source_label,
                        "id": link,
                        "title": title,
                        "date": clean_date,
                        "link": link
                    })
                    seen_ids.add(link)
                    found_any_in_page = True

                except Exception:
                    continue

            # --- BACKUP: N·∫æU KH√îNG TH·∫§Y NG√ÄY ---
            # N·∫øu trang n√†y kh√¥ng t√¨m ƒë∆∞·ª£c tin n√†o theo ng√†y (c√≥ th·ªÉ do layout kh√°c), qu√©t theo Title
            if not found_any_in_page:
                all_links = soup.find_all('a', href=True)
                for a in all_links:
                    link = a['href']
                    title = a.get_text(strip=True)
                    if not title: title = a.get('title', '')
                    
                    if len(title) > 10 and str(current_year) in title:
                        if link.startswith('/'): link = base_url + link
                        if link in seen_ids: continue
                        
                        new_items.append({
                            "source": source_label,
                            "id": link,
                            "title": title,
                            "date": f"01/01/{current_year}",
                            "link": link
                        })
                        seen_ids.add(link)

        except Exception as e:
            print(f"   ! L·ªói khi qu√©t {source_label}: {e}")

    return new_items